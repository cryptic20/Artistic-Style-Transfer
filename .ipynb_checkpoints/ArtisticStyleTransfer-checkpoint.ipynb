{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gram Matrix Layer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class GramMatrix(nn.Module):\n",
    "    \n",
    "    def forward(self, input):\n",
    "        a, b, c, d = input.size()\n",
    "        features = input.view(a * b, c * d)\n",
    "        G = torch.mm(features, features.t())\n",
    "        \n",
    "        return G.div(a * b * c * d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Style CNN\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "\n",
    "class StyleCNN(object):\n",
    "    def __init__(self, style, content, pastiche):\n",
    "        super(StyleCNN, self).__init__()\n",
    "        \n",
    "        self.style = style\n",
    "        self.content = content\n",
    "        self.pastiche = nn.Parameter(pastiche.data)\n",
    "        \n",
    "        self.content_layers = ['conv_4']\n",
    "        self.style_layers = ['conv_1', 'conv_2', 'conv_3', 'conv_4', 'conv_5']\n",
    "        self.content_weight = 1\n",
    "        self.style_weight = 1000\n",
    "        \n",
    "        self.loss_network = models.vgg19(pretrained=True)\n",
    "        \n",
    "        self.gram = GramMatrix()\n",
    "        self.loss = nn.MSELoss()\n",
    "        self.optimizer = optim.LBFGS([self.pastiche])\n",
    "        \n",
    "        self.use_cuda = torch.cuda.is_available()\n",
    "        if self.use_cuda:\n",
    "            self.loss_network.cuda()\n",
    "            self.gram.cuda()\n",
    "            \n",
    "    # training\n",
    "    \n",
    "    def train(self):\n",
    "        def closure():\n",
    "            self.optimizer.zero_grad()\n",
    "          \n",
    "            pastiche = self.pastiche.clone()\n",
    "            pastiche.data.clamp_(0, 1)\n",
    "            content = self.content.clone()\n",
    "            style = self.style.clone()\n",
    "            \n",
    "            content_loss = 0\n",
    "            style_loss = 0\n",
    "            \n",
    "            i = 1\n",
    "            not_inplace = lambda layer: nn.ReLU(inplace=False) if isinstance(layer, nn.ReLU) else layer\n",
    "            for layer in list(self.loss_network.features):\n",
    "                layer = not_inplace(layer)\n",
    "                if self.use_cuda:\n",
    "                    layer.cuda()\n",
    "                    \n",
    "                pastiche, content, style = layer.forward(pastiche), layer.forward(content), layer.forward(style)\n",
    "                \n",
    "                if isinstance(layer, nn.Conv2d):\n",
    "                    name = \"conv_\" + str(i)\n",
    "                    \n",
    "                    if name in self.content_layers:\n",
    "                        content_loss += self.loss(pastiche * self.content_weight, content.detach() * self.content_weight)\n",
    "                    \n",
    "                    if name in self.style_layers:\n",
    "                        pastiche_g, style_g = self.gram.forward(pastiche), self.gram.forward(style)\n",
    "                        style_loss += self.loss(pastiche_g * self.style_weight, style_g.detach() * self.style_weight)\n",
    "                \n",
    "                if isinstance(layer, nn.ReLU):\n",
    "                    i += 1\n",
    "            \n",
    "            total_loss = content_loss + style_loss\n",
    "            total_loss.backward()\n",
    "            \n",
    "            return total_loss\n",
    "    \n",
    "        self.optimizer.step(closure)\n",
    "        return self.pastiche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utility functions\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from PIL import Image\n",
    "import scipy.misc\n",
    "\n",
    "imsize = 256\n",
    "\n",
    "loader = transforms.Compose([\n",
    "             transforms.Resize(imsize),\n",
    "             transforms.ToTensor()\n",
    "         ])\n",
    "\n",
    "unloader = transforms.ToPILImage()\n",
    "\n",
    "def image_loader(image_name):\n",
    "    image = Image.open(image_name)\n",
    "    image = Variable(loader(image))\n",
    "    image = image.unsqueeze(0)\n",
    "    return image\n",
    "  \n",
    "def save_image(input, path):\n",
    "    image = input.data.clone().cpu()\n",
    "    image = image.view(3, imsize, imsize)\n",
    "    image = unloader(image)\n",
    "    scipy.misc.imsave(path, image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-948122c5b197>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mpastiche\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimage_loader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"contents/dancer.jpg\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mpastiche\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m31\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'function' object has no attribute 'data'"
     ]
    }
   ],
   "source": [
    "# main \n",
    "import torch.utils.data\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "\n",
    "# CUDA Configurations\n",
    "dtype = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor\n",
    "\n",
    "# Content and style\n",
    "style = image_loader(\"Styles/starrysky.jpg\").type(dtype)\n",
    "content = image_loader(\"Contents/dancer.jpg\").type(dtype)\n",
    "\n",
    "pastiche = image_loader(\"contents/dancer.jpg\").type(dtype)\n",
    "pastiche.data = torch.randn(input.data.size()).type(dtype)\n",
    "\n",
    "num_epochs = 31\n",
    "\n",
    "def main():\n",
    "    style_cnn = StyleCNN(style, content, pastiche)\n",
    "    \n",
    "    for i in range(num_epochs):\n",
    "        pastiche = style_cnn.train()\n",
    "    \n",
    "        if i % 10 == 0:\n",
    "            print(\"Iteration: %d\" % (i))\n",
    "            \n",
    "            path = \"outputs/%d.png\" % (i)\n",
    "            pastiche.data.clamp_(0, 1)\n",
    "            save_image(pastiche, path)\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
